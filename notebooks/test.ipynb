{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ecb9b5-450d-4007-9750-e47af9860078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b40a16-62e8-43d0-bd4c-c904b05deca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'coding': 3, 'love': 4, 'hate': 5, 'enjoy': 6, 'learning': 7, 'new': 8, 'technologies': 9, 'dislike': 10, 'debugging': 11, 'late': 12, 'at': 13, 'night': 14}\n",
      "[[2, 4, 3], [2, 5, 3], [2, 6, 7, 8, 9], [2, 10, 11, 12, 13, 14]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'I love coding',\n",
    "    'I hate coding',\n",
    "    'I enjoy learning new technologies',\n",
    "    'I dislike debugging late at night'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766800e9-542a-4ca0-97bd-9e729f10e54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 1, 1], [2, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    'I admire creative ideas',\n",
    "    'I avoid complex problems'\n",
    "]\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "print(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163b8831-5b07-4142-b75d-bf021c6c3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = \"\"\"I don't know who to trust, no surprise\n",
    "Everyone feels so far away from me\n",
    "Heavy thoughts sift through dust and the lies\n",
    "\n",
    "Tryin' not to break, but I'm so tired of this deceit\n",
    "Every time I try to make myself get back up on my feet\n",
    "(I) All I ever think about is this, all the tiring time between\n",
    "And how trying to put my trust in you just takes so much out of me\n",
    "\n",
    "Take everything from the inside\n",
    "And throw it all away\n",
    "'Cause I swear, for the last time\n",
    "I won't trust myself with you\n",
    "\n",
    "Tension is building inside, steadily\n",
    "Everyone feels so far away from me\n",
    "Heavy thoughts forcing their way out of me\n",
    "\n",
    "Tryin' not to break, but I'm so tired of this deceit\n",
    "Every time I try to make myself get back up on my feet\n",
    "(I) All I ever think about is this, all the tiring time between\n",
    "And how trying to put my trust in you just takes so much out of me\n",
    "\n",
    "Take everything from the inside\n",
    "And throw it all away\n",
    "'Cause I swear, for the last time\n",
    "I won't trust myself with you\n",
    "\n",
    "I won't waste myself on you, you, you\n",
    "Waste myself on you, you, you\n",
    "\n",
    "I'll take everything from the inside\n",
    "And throw it all away\n",
    "'Cause I swear, for the last time\n",
    "I won't trust myself with you\n",
    "Everything from the inside\n",
    "And just throw it all away\n",
    "'Cause I swear, for the last time\n",
    "I won't trust myself with you, you, you\"\"\"\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4f0266-04b2-4160-9bba-8630d14b5bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f880c67b-eaf3-419e-9f84-b0849a004053",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd14609-c08f-4b5b-8423-ac102877765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7deac50e-475f-4e06-9561-b3fc80859f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "xs = input_sequences[:,:-1]\n",
    "labels = input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9dbcb8f-fec3-4c7c-9243-6e6fc790ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54ca73c-923b-4d3f-8c78-faabede0e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0bbd6b-68b5-428e-abd0-01482de7a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(total_words, 240, input_length=max_sequence_len-1),\n",
    "    layers.Bidirectional(layers.LSTM(256)),\n",
    "    layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85f0e19-a6ea-4808-80b4-737a9f445672",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xs, ys, epochs=150, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4ca100-a7b0-4985-88e5-d1f59f5f8ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't know who to trust no surprise\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"i don't know\"\n",
    "next_words = 5\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_index = np.argmax(predicted, axis=-1)[0]\n",
    "    \n",
    "    output_word = tokenizer.index_word[predicted_index]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fbd34-b52e-48de-90e2-1de497f79281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
